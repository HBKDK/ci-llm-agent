"""
LangGraph ê¸°ë°˜ CI ì˜¤ë¥˜ ë¶„ì„ ì›Œí¬í”Œë¡œìš°
"""
import os
from typing import Dict, List, TypedDict, Annotated, Any
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
# DuckDuckGo ë„êµ¬ ì œê±° - ìë™ì°¨ SW íŠ¹í™” ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‚¬ìš©

from app.kb.db import ensure_initialized, search_kb
from app.utils.text import extract_symptoms, truncate_tokens


class CIWorkflowState(TypedDict, total=False):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜ - total=Falseë¡œ ëª¨ë“  í‚¤ë¥¼ ì„ íƒì ìœ¼ë¡œ ì„¤ì •"""
    # ì…ë ¥
    ci_log: str
    context: str
    repository: str
    
    # ì¤‘ê°„ ê²°ê³¼
    symptoms: List[str]
    kb_hits: List[Dict]
    web_hits: List[Dict]
    
    # ë³´ì•ˆ ê´€ë ¨
    security_status: str
    kb_confidence: float
    
    # ìµœì¢… ê²°ê³¼
    analysis: str
    confidence: float
    error_type: str


class CIErrorAnalyzer:
    """CI ì˜¤ë¥˜ ë¶„ì„ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self):
        self.llm = self._get_llm()
        # DuckDuckGo ë„êµ¬ ì œê±° - ìƒˆë¡œìš´ ìë™ì°¨ SW íŠ¹í™” ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‚¬ìš©
        ensure_initialized()
    
    def _get_llm(self):
        """LLM ì´ˆê¸°í™”"""
        llm_provider = os.getenv("LLM_PROVIDER", "").lower()
        
        # LLM_PROVIDERê°€ ë¹„ì–´ìˆìœ¼ë©´ LLM ë¹„í™œì„±í™”
        if not llm_provider:
            return None
        
        if llm_provider == "private":
            # Private LLM
            base_url = os.getenv("PRIVATE_LLM_BASE_URL")
            if not base_url:
                print("âš ï¸ PRIVATE_LLM_BASE_URL ë¯¸ì„¤ì •, LLM ë¹„í™œì„±í™”")
                return None
            
            return ChatOpenAI(
                model=os.getenv("PRIVATE_LLM_MODEL", "llama-3-70b"),
                temperature=0.2,
                openai_api_key=os.getenv("PRIVATE_LLM_API_KEY", "not-needed"),
                openai_api_base=base_url
            )
        else:
            # OpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì •, LLM ë¹„í™œì„±í™”")
                return None
            
            return ChatOpenAI(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                temperature=0.2,
                api_key=api_key
            )
        
        return None
    
    def extract_symptoms_node(self, state: CIWorkflowState) -> Dict:
        """ì¦ìƒ ì¶”ì¶œ ë…¸ë“œ"""
        symptoms = extract_symptoms(state["ci_log"])
        
        # ì˜¤ë¥˜ íƒ€ì… ë¶„ë¥˜
        error_type = self._classify_error_type(symptoms)
        
        return {
            "symptoms": symptoms,
            "error_type": error_type
        }
    
    def search_knowledge_base_node(self, state: CIWorkflowState) -> Dict:
        """ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ë…¸ë“œ"""
        query = "\n".join(state["symptoms"])
        kb_hits = search_kb(query=query, top_k=5)
        
        # KB ì‹ ë¢°ë„ ê³„ì‚°
        kb_confidence = self._calculate_kb_confidence(kb_hits)
        
        return {
            "kb_hits": kb_hits,
            "kb_confidence": kb_confidence
        }
    
    # ì›¹ ê²€ìƒ‰ ë…¸ë“œ ì œê±° (í˜„ì¬ ë²„ì „ì—ì„œëŠ” KB ê²€ìƒ‰ë§Œ ì‚¬ìš©)
    # TODO: í–¥í›„ í•„ìš”ì‹œ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€ ê°€ëŠ¥
    
    def _calculate_kb_confidence(self, kb_hits: List[Dict]) -> float:
        """KB ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not kb_hits:
            return 0.0
        
        # KB ê²°ê³¼ì˜ ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        total_score = sum(hit.get("score", 0) for hit in kb_hits)
        avg_score = total_score / len(kb_hits)
        
        # KB ê²°ê³¼ ê°œìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        count_bonus = min(len(kb_hits) * 0.1, 0.3)
        
        return min(avg_score + count_bonus, 1.0)
    
    # ì›¹ ê²€ìƒ‰ ë³´ì•ˆ ë©”ì„œë“œë“¤ (í˜„ì¬ ë¯¸ì‚¬ìš© - í–¥í›„ ì›¹ ê²€ìƒ‰ ì¶”ê°€ ì‹œ í™œì„±í™”)
    
    def _is_search_safe(self, keywords: List[str], error_type: str) -> bool:
        """ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ë³´ì•ˆ ê²€ì¦"""
        # ë³´ì•ˆ ë¯¼ê° í‚¤ì›Œë“œ ëª©ë¡
        sensitive_keywords = [
            "password", "secret", "key", "token", "credential",
            "auth", "login", "admin", "root", "access",
            "security", "encryption", "certificate", "ssl", "tls",
            "vulnerability", "exploit", "hack", "attack", "breach",
            "confidential", "proprietary", "internal", "private"
        ]
        
        # ì‹œìŠ¤í…œ ì •ë³´ ë…¸ì¶œ í‚¤ì›Œë“œ
        system_keywords = [
            "path", "directory", "file", "config", "setting",
            "environment", "variable", "server", "host", "ip",
            "database", "connection", "url", "endpoint"
        ]
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
        query_text = " ".join(keywords).lower()
        
        # 1. ë³´ì•ˆ ë¯¼ê° í‚¤ì›Œë“œ ê²€ì‚¬
        for sensitive in sensitive_keywords:
            if sensitive in query_text:
                print(f"ë³´ì•ˆ ë¯¼ê° í‚¤ì›Œë“œ ê°ì§€: {sensitive}")
                return False
        
        # 2. ì‹œìŠ¤í…œ ì •ë³´ ë…¸ì¶œ í‚¤ì›Œë“œ ê²€ì‚¬ (ê²½ê³  ìˆ˜ì¤€)
        system_count = sum(1 for sys_kw in system_keywords if sys_kw in query_text)
        if system_count >= 3:  # 3ê°œ ì´ìƒ ì‹œìŠ¤í…œ í‚¤ì›Œë“œ
            print(f"ì‹œìŠ¤í…œ ì •ë³´ ë…¸ì¶œ ìœ„í—˜: {system_count}ê°œ í‚¤ì›Œë“œ")
            return False
        
        # 3. ìë™ì°¨ SW ë„ë©”ì¸ íŠ¹í™” ë³´ì•ˆ ê²€ì‚¬
        if not self._is_automotive_sw_related(keywords, error_type):
            print("ìë™ì°¨ SW ê´€ë ¨ì„± ë¶€ì¡±, ê²€ìƒ‰ ì°¨ë‹¨")
            return False
        
        return True
    
    def _is_automotive_sw_related(self, keywords: List[str], error_type: str) -> bool:
        """ìë™ì°¨ SW ê´€ë ¨ì„± ê²€ì¦"""
        automotive_terms = [
            "tasking", "nxp", "polyspace", "simulink", "autosar",
            "can", "canoe", "vector", "davinci", "misra", "iso26262",
            "c166", "c251", "carm", "s32k", "s32", "tricore", "aurix",
            "compiler", "build", "linker", "assembler", "code generation",
            "automotive", "embedded", "ecu", "mcu", "vehicle"
        ]
        
        query_text = " ".join(keywords).lower()
        
        # ìë™ì°¨ SW ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        automotive_count = sum(1 for term in automotive_terms if term in query_text)
        
        # ìµœì†Œ 1ê°œ ì´ìƒì˜ ìë™ì°¨ SW ê´€ë ¨ í‚¤ì›Œë“œ í•„ìš”
        return automotive_count >= 1
    
    def _filter_sensitive_content(self, web_hits: List[Dict]) -> List[Dict]:
        """ì›¹ ê²€ìƒ‰ ê²°ê³¼ì˜ ë¯¼ê°í•œ ë‚´ìš© í•„í„°ë§"""
        filtered_hits = []
        
        for hit in web_hits:
            title = hit.get("title", "").lower()
            snippet = hit.get("snippet", "").lower()
            url = hit.get("url", "").lower()
            
            # ë¯¼ê°í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²°ê³¼ ì œì™¸
            sensitive_patterns = [
                "password", "secret", "key", "token", "credential",
                "admin", "root", "login", "auth", "security",
                "vulnerability", "exploit", "hack", "attack",
                "confidential", "proprietary", "internal"
            ]
            
            is_sensitive = any(pattern in title or pattern in snippet or pattern in url 
                             for pattern in sensitive_patterns)
            
            if not is_sensitive:
                filtered_hits.append(hit)
            else:
                print(f"ë¯¼ê°í•œ ë‚´ìš© ê°ì§€, ê²°ê³¼ ì œì™¸: {hit.get('title', '')[:50]}...")
        
        return filtered_hits
    
    # ì›¹ ê²€ìƒ‰ ë©”ì„œë“œë“¤ (í˜„ì¬ ë¹„í™œì„±í™” - KBë§Œ ì‚¬ìš©)
    # TODO: ë‚˜ì¤‘ì— ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€ ì‹œ í™œì„±í™”
    
    def analyze_with_llm_node(self, state: CIWorkflowState) -> Dict:
        """LLM ë¶„ì„ ë…¸ë“œ"""
        if not self.llm:
            # LLMì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ë¶„ì„ ì œê³µ
            return {
                "analysis": self._fallback_analysis(state),
                "confidence": 0.7
            }
        
        prompt = self._build_analysis_prompt(state)
        
        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ CI/CD ì˜¤ë¥˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìƒì„¸í•œ ë¶„ì„ê³¼ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”."),
                HumanMessage(content=prompt)
            ]
            
            response = self.llm.invoke(messages)
            return {
                "analysis": response.content,
                "confidence": self._calculate_confidence(state)
            }
            
        except Exception as e:
            print(f"LLM ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "analysis": self._fallback_analysis(state),
                "confidence": 0.7
            }
    
    def _classify_error_type(self, symptoms: List[str]) -> str:
        """ìë™ì°¨ SW ê°œë°œ í™˜ê²½ ì˜¤ë¥˜ íƒ€ì… ë¶„ë¥˜"""
        error_keywords = {
            "tasking": [
                "tasking", "c166", "c251", "carm", "compiler error", 
                "tasking compiler", "code generation", "assembler error",
                "linker error", "tasking ide", "tricore", "aurix"
            ],
            "nxp": [
                "nxp", "s32", "s32k", "nxp compiler", "s32 design studio",
                "nxp mcu", "nxp ide", "nxp toolchain", "nxp debugger"
            ],
            "polyspace": [
                "polyspace", "static analysis", "code verification", 
                "polyspace bug finder", "polyspace code prover",
                "misra", "cert", "iso 26262", "polyspace error"
            ],
            "simulink": [
                "simulink", "matlab", "stateflow", "simulink error",
                "model compilation", "code generation", "targetlink",
                "embedded coder", "simulink build"
            ],
            "autosar": [
                "autosar", "vector", "davinci", "autosar cp", "autosar ap",
                "ecu extract", "bsw", "rte", "autosar configuration",
                "vector canoe", "vector cast", "autosar toolchain"
            ],
            "can": [
                "can", "canoe", "canape", "can bus", "can message",
                "can signal", "dbc", "can error", "can communication",
                "vector canoe", "peak can", "canalyzer"
            ],
            "compilation": [
                "compilation error", "build error", "make error", "makefile",
                "gcc", "g++", "compiler", "linker", "assembler",
                "build failed", "compilation failed"
            ],
            "ci": [
                "jenkins", "gitlab ci", "github actions", "azure devops",
                "pipeline", "build pipeline", "continuous integration"
            ]
        }
        
        symptoms_text = " ".join(symptoms).lower()
        
        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ê²€ì‚¬ (íŠ¹ì • ë„êµ¬ê°€ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë³´ë‹¤ ìš°ì„ )
        priority_order = ["tasking", "nxp", "polyspace", "simulink", "autosar", "can", "compilation", "ci"]
        
        for error_type in priority_order:
            if error_type in error_keywords:
                keywords = error_keywords[error_type]
                if any(keyword.lower() in symptoms_text for keyword in keywords):
                    return error_type
        
        return "unknown"
    
    def _build_analysis_prompt(self, state: CIWorkflowState) -> str:
        """ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        kb_section = "\n\n".join([
            f"[KB#{i+1}] {hit['title']}\nìš”ì•½: {hit['summary']}\ní•´ê²°ì±…: {hit['fix']}"
            for i, hit in enumerate(state["kb_hits"])
        ]) or "(ì§€ì‹ë² ì´ìŠ¤ ê²°ê³¼ ì—†ìŒ)"
        
        web_section = "\n\n".join([
            f"[WEB#{i+1}] {hit['title']}\në‚´ìš©: {hit['snippet']}"
            for i, hit in enumerate(state["web_hits"])
        ]) or "(ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"
        
        trimmed_log = truncate_tokens(state["ci_log"], max_chars=4000)
        
        return f"""
ë‹¤ìŒ CI ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ê³  í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”:

**ì»¨í…ìŠ¤íŠ¸:**
- ì €ì¥ì†Œ: {state.get("repository", "ì•Œ ìˆ˜ ì—†ìŒ")}
- ì¶”ê°€ ì •ë³´: {state.get("context", "ì—†ìŒ")}
- ì˜¤ë¥˜ ìœ í˜•: {state["error_type"]}

**CI ë¡œê·¸:**
{trimmed_log}

**ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼:**
{kb_section}

**ì›¹ ê²€ìƒ‰ ê²°ê³¼:**
{web_section}

**ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
1. ì˜¤ë¥˜ì˜ í•µì‹¬ ì›ì¸ì„ 1-2ì¤„ë¡œ ìš”ì•½
2. ê´€ë ¨ ê·¼ê±°ë¥¼ KB/WEB ë²ˆí˜¸ë¡œ ëª…ì‹œ
3. ë‹¨ê³„ë³„ í•´ê²°ì±… ì œì‹œ (ëª…ë ¹ì–´/ì„¤ì • ì˜ˆì‹œ í¬í•¨)
4. ì¬ë°œ ë°©ì§€ ë°©ë²• ì œì•ˆ
5. ì¶”ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì œê³µ

í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    
    def _fallback_analysis(self, state: CIWorkflowState) -> str:
        """LLM ì—†ì„ ë•Œ ëŒ€ì²´ ë¶„ì„"""
        symptoms = state["symptoms"]
        error_type = state["error_type"]
        
        return f"""## ğŸ” ì˜¤ë¥˜ ë¶„ì„ (ë¡œì»¬ ë¶„ì„)

**ì˜¤ë¥˜ ìœ í˜•**: {error_type}
**í•µì‹¬ ì¦ìƒ**: {symptoms[0] if symptoms else "ë¶„ì„ ë¶ˆê°€"}

## ğŸ› ï¸ ì¼ë°˜ì  í•´ê²°ì±…

### 1ë‹¨ê³„: ê¸°ë³¸ í™•ì¸
- ì˜ì¡´ì„± íŒŒì¼ ì¡´ì¬ í™•ì¸
- íŒ¨í‚¤ì§€ ì„¤ì¹˜ ìƒíƒœ í™•ì¸
- ë²„ì „ í˜¸í™˜ì„± í™•ì¸

### 2ë‹¨ê³„: ì¬ì„¤ì¹˜ ì‹œë„
```bash
# Pythonì˜ ê²½ìš°
pip install -r requirements.txt

# Node.jsì˜ ê²½ìš°  
npm install

# Dockerì˜ ê²½ìš°
docker build --no-cache .
```

### 3ë‹¨ê³„: ìºì‹œ ì •ë¦¬
- pip cache clear
- npm cache clean
- Docker system prune

**ì°¸ê³ **: ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.
"""
    
    def _calculate_confidence(self, state: CIWorkflowState) -> float:
        """ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # KB ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
        if state["kb_hits"]:
            confidence += 0.3
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
        if state["web_hits"]:
            confidence += 0.2
        
        return min(confidence, 1.0)
    
    def should_continue(self, state: CIWorkflowState) -> str:
        """ì›Œí¬í”Œë¡œìš° ê³„ì† ì—¬ë¶€ ê²°ì •"""
        if state["symptoms"]:
            return "continue"
        return END
    
    def create_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        workflow = StateGraph(CIWorkflowState)
        
        # ë…¸ë“œ ì¶”ê°€ (ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”)
        workflow.add_node("extract_symptoms", self.extract_symptoms_node)
        workflow.add_node("search_kb", self.search_knowledge_base_node)
        workflow.add_node("analyze_llm", self.analyze_with_llm_node)
        
        # ì—£ì§€ ì¶”ê°€ (ë‹¨ìˆœí™”: ì¦ìƒ ì¶”ì¶œ â†’ KB ê²€ìƒ‰ â†’ LLM ë¶„ì„)
        workflow.set_entry_point("extract_symptoms")
        workflow.add_edge("extract_symptoms", "search_kb")
        workflow.add_edge("search_kb", "analyze_llm")
        workflow.add_edge("analyze_llm", END)
        
        return workflow.compile()


def create_ci_analyzer() -> CIErrorAnalyzer:
    """CI ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return CIErrorAnalyzer()


def run_analysis(ci_log: str, context: str | None = None, repository: str | None = None) -> Dict:
    """CI ì˜¤ë¥˜ ë¶„ì„ ì‹¤í–‰"""
    analyzer = create_ci_analyzer()
    workflow = analyzer.create_workflow()
    
    initial_state = {
        "ci_log": ci_log,
        "context": context or "",
        "repository": repository or "",
        "symptoms": [],
        "kb_hits": [],
        "web_hits": [],  # ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”
        "security_status": "web_disabled",  # ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì•ˆ í•¨
        "kb_confidence": 0.0,
        "analysis": "",
        "confidence": 0.0,
        "error_type": "unknown"
    }
    
    result = workflow.invoke(initial_state)
    
    return {
        "symptoms": result["symptoms"],
        "kb_hits": result["kb_hits"],
        "web_hits": result["web_hits"],
        "security_status": result["security_status"],
        "kb_confidence": result["kb_confidence"],
        "analysis": result["analysis"],
        "confidence": result["confidence"],
        "error_type": result["error_type"]
    }
