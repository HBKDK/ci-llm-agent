import os
from dataclasses import dataclass
from typing import Protocol

from tenacity import retry, stop_after_attempt, wait_exponential
from app.utils.config import load_config, get_azure_config, get_private_llm_config, get_openai_config


class LLMClient(Protocol):
    async def achain(self, prompt: str) -> str:  # simple async generate
        ...


@dataclass
class OpenAIClient:
    model: str = "gpt-4o-mini"

    @retry(wait=wait_exponential(multiplier=1, min=1, max=20), stop=stop_after_attempt(3))
    async def achain(self, prompt: str) -> str:
        from openai import AsyncOpenAI

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            # fallback: local simple heuristic summarizer
            return _local_fallback(prompt)
        client = AsyncOpenAI(api_key=api_key)
        resp = await client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ DevOps ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        return resp.choices[0].message.content or ""


@dataclass
class AzureOpenAIClient:
    """Azure OpenAI í´ë¼ì´ì–¸íŠ¸"""
    endpoint: str = ""
    deployment_name: str = ""
    api_key: str = ""
    api_version: str = "2024-02-15-preview"

    @retry(wait=wait_exponential(multiplier=1, min=1, max=20), stop=stop_after_attempt(3))
    async def achain(self, prompt: str) -> str:
        from openai import AsyncOpenAI

        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        client = AsyncOpenAI(
            api_key=self.api_key,
            azure_endpoint=self.endpoint,
            api_version=self.api_version
        )
        
        resp = await client.chat.completions.create(
            model=self.deployment_name,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ìë™ì°¨ SW ê°œë°œ í™˜ê²½ì˜ DevOps ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        return resp.choices[0].message.content or ""


@dataclass
class PrivateLLMClient:
    """Private LLM (ì‚¬ë‚´ LLM ì„œë²„) í´ë¼ì´ì–¸íŠ¸"""
    base_url: str = ""
    model: str = ""
    api_key: str = ""

    @retry(wait=wait_exponential(multiplier=1, min=1, max=20), stop=stop_after_attempt(3))
    async def achain(self, prompt: str) -> str:
        from openai import AsyncOpenAI

        # Private LLMë„ OpenAI API í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
        client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        resp = await client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ìë™ì°¨ SW ê°œë°œ í™˜ê²½ì˜ DevOps ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        return resp.choices[0].message.content or ""


def _local_fallback(prompt: str) -> str:
    # ê°„ë‹¨í•œ ì¶”ì¶œ ìš”ì•½ (LLM í‚¤ê°€ ì—†ì„ ë•Œ ë¹„ìƒ ë™ì‘)
    import re

    lines = [l.strip() for l in prompt.splitlines() if l.strip()]
    candidates = [l for l in lines if any(k in l.lower() for k in ["error", "exception", "failed", "unable", "missing", "not found"])][:5]
    summary = "\n".join(candidates) or lines[-10:]
    return (
        "[ë¡œì»¬ ìš”ì•½] LLM í‚¤ê°€ ì—†ì–´ ê°„ì´ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.\n" +
        "í•µì‹¬ ë©”ì‹œì§€:\n" + "\n".join(candidates[:3]) + "\n\n" +
        "ê°€ëŠ¥í•œ ì¡°ì¹˜:\n- ë¡œê·¸ì˜ íŒ¨í‚¤ì§€/ë²„ì „ ì¶©ëŒ í™•ì¸\n- ìºì‹œ/ë¹Œë“œ í´ë” ì •ë¦¬ í›„ ì¬ì‹œë„\n- ì˜ì¡´ì„± ì„¤ì¹˜ ë‹¨ê³„ ì¬ì‹¤í–‰\n- ê´€ë ¨ ì´ìŠˆ ê²€ìƒ‰ìœ¼ë¡œ í•´ê²°ì±… í™•ì¸"
    ).strip()


def get_llm() -> LLMClient:
    """
    LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± (config íŒŒì¼ + í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
    
    ìš°ì„ ìˆœìœ„: í™˜ê²½ë³€ìˆ˜ > config íŒŒì¼ > ê¸°ë³¸ê°’
    
    í™˜ê²½ë³€ìˆ˜:
        LLM_PROVIDER: openai (ê¸°ë³¸ê°’), azure, ë˜ëŠ” private
        
        OpenAI ì‚¬ìš© ì‹œ:
            OPENAI_API_KEY: OpenAI API í‚¤
        
        Azure OpenAI ì‚¬ìš© ì‹œ:
            AZURE_OPENAI_ENDPOINT: Azure OpenAI ì—”ë“œí¬ì¸íŠ¸ URL
            AZURE_OPENAI_DEPLOYMENT_NAME: ë°°í¬ëœ ëª¨ë¸ ì´ë¦„
            AZURE_OPENAI_API_KEY: Azure OpenAI API í‚¤
            AZURE_OPENAI_API_VERSION: API ë²„ì „ (ê¸°ë³¸ê°’: 2024-02-15-preview)
        
        Private LLM ì‚¬ìš© ì‹œ:
            PRIVATE_LLM_BASE_URL: Private LLM ì„œë²„ URL (ì˜ˆ: http://llm-server:8000/v1)
            PRIVATE_LLM_MODEL: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: llama-3-70b, mistral-7b)
            PRIVATE_LLM_API_KEY: Private LLM API í‚¤ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    """
    # config íŒŒì¼ ë¡œë“œ ì‹œë„
    try:
        config = load_config()
        print("âœ… Config íŒŒì¼ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âš ï¸ Config íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        config = {}
    
    llm_provider = os.getenv("LLM_PROVIDER", "openai").lower()
    
    if llm_provider == "azure":
        # Azure OpenAI ì‚¬ìš© - config íŒŒì¼ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        azure_config = get_azure_config(config)
        endpoint = azure_config["endpoint"]
        deployment_name = azure_config["deployment_name"]
        api_key = azure_config["api_key"]
        api_version = azure_config["api_version"]
        
        print(f"ğŸ”§ Azure OpenAI ì„¤ì •:")
        print(f"   Endpoint: {endpoint}")
        print(f"   Deployment: {deployment_name}")
        print(f"   API Key: {'ì„¤ì •ë¨' if api_key else 'ì—†ìŒ'}")
        print(f"   API Version: {api_version}")
        
        if not endpoint or not deployment_name or not api_key:
            print("âš ï¸ Azure OpenAI ì„¤ì •ì´ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ë¡œì»¬ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            print(f"   í•„ìš”í•œ ì„¤ì •: endpoint, deployment_name, api_key")
            # Fallbackì„ ë°˜í™˜í•˜ëŠ” ê°„ë‹¨í•œ í´ë˜ìŠ¤
            class LocalClient:
                async def achain(self, prompt: str) -> str:
                    return _local_fallback(prompt)
            return LocalClient()
        
        return AzureOpenAIClient(
            endpoint=endpoint,
            deployment_name=deployment_name,
            api_key=api_key,
            api_version=api_version
        )
    
    elif llm_provider == "private":
        # Private LLM ì‚¬ìš© - config íŒŒì¼ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        private_config = get_private_llm_config(config)
        base_url = private_config["base_url"]
        model = private_config["model"]
        api_key = private_config["api_key"]
        
        print(f"ğŸ”§ Private LLM ì„¤ì •:")
        print(f"   Base URL: {base_url}")
        print(f"   Model: {model}")
        print(f"   API Key: {'ì„¤ì •ë¨' if api_key else 'ì—†ìŒ'}")
        
        if not base_url:
            print("âš ï¸ PRIVATE_LLM_BASE_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # Fallbackì„ ë°˜í™˜í•˜ëŠ” ê°„ë‹¨í•œ í´ë˜ìŠ¤
            class LocalClient:
                async def achain(self, prompt: str) -> str:
                    return _local_fallback(prompt)
            return LocalClient()
        
        return PrivateLLMClient(
            base_url=base_url,
            model=model,
            api_key=api_key
        )
    
    else:
        # OpenAI ì‚¬ìš© (ê¸°ë³¸ê°’) - config íŒŒì¼ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        openai_config = get_openai_config(config)
        model = openai_config["model"]
        api_key = openai_config["api_key"]
        
        print(f"ğŸ”§ OpenAI ì„¤ì •:")
        print(f"   Model: {model}")
        print(f"   API Key: {'ì„¤ì •ë¨' if api_key else 'ì—†ìŒ'}")
        
        return OpenAIClient(model=model)


