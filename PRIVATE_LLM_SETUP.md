# π”§ Private LLM λ° IP κΈ°λ° μ ‘μ† μ„¤μ • κ°€μ΄λ“

## π― Private LLM μ„¤μ •

### 1οΈβƒ£ **ν™κ²½λ³€μ μ„¤μ •**

#### k8s/secrets.yaml μμ •

```yaml
# ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # Private LLM μ„λ²„ μ„¤μ •
  private-llm-base-url: "http://your-llm-server.company.com:8000/v1"
  private-llm-model: "llama-3-70b"  # μ‚¬μ©ν•  λ¨λΈ μ΄λ¦„
  
  # λλ” K8s λ‚΄λ¶€ μ„λΉ„μ¤
  # private-llm-base-url: "http://llm-service:8000/v1"

---
# Secret
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
type: Opaque
stringData:
  # Private LLM API ν‚¤ (ν•„μ”ν• κ²½μ°)
  private-llm-api-key: "your-private-llm-api-key"
  
  # JWT Secret
  jwt-secret-key: "change-this-jwt-secret-in-production"
```

#### k8s/deployment.yamlμ—μ„ LLM_PROVIDER μ„¤μ •

```yaml
env:
- name: LLM_PROVIDER
  value: "private"  # private λλ” openai
```

### 2οΈβƒ£ **μ§€μ›ν•λ” Private LLM**

Private LLM μ„λ²„κ°€ **OpenAI API νΈν™ μΈν„°νμ΄μ¤**λ¥Ό μ κ³µν•λ©΄ λ¨λ‘ μ‚¬μ© κ°€λ¥ν•©λ‹λ‹¤:

#### β… **νΈν™λλ” LLM μ„λ²„**
- **vLLM**: OpenAI API νΈν™ μ„λ²„
- **Text Generation Inference (TGI)**: Hugging Face
- **Ollama**: OpenAI API νΈν™ λ¨λ“
- **LM Studio**: OpenAI API νΈν™
- **FastChat**: OpenAI API νΈν™
- **LocalAI**: OpenAI API νΈν™

#### π“ **Private LLM μ„λ²„ μμ‹**

```bash
# vLLM μ„λ²„ μ‹¤ν–‰ μμ‹
python -m vllm.entrypoints.openai.api_server \
  --model llama-3-70b \
  --host 0.0.0.0 \
  --port 8000

# Ollama OpenAI νΈν™ λ¨λ“
OLLAMA_HOST=0.0.0.0:8000 ollama serve
```

### 3οΈβƒ£ **μ„¤μ • μμ‹**

#### μμ‹ 1: vLLM μ„λ²„ μ‚¬μ©
```yaml
# ConfigMap
data:
  private-llm-base-url: "http://vllm-server.company.com:8000/v1"
  private-llm-model: "llama-3-70b"

# Secret
stringData:
  private-llm-api-key: "not-needed"  # vLLMμ€ API ν‚¤ λ¶ν•„μ”
```

#### μμ‹ 2: μ‚¬λ‚΄ LLM μ„λΉ„μ¤
```yaml
# ConfigMap
data:
  private-llm-base-url: "http://llm-api.internal.company.com/v1"
  private-llm-model: "company-custom-model"

# Secret
stringData:
  private-llm-api-key: "internal-api-key-12345"
```

#### μμ‹ 3: K8s λ‚΄λ¶€ LLM μ„λΉ„μ¤
```yaml
# ConfigMap
data:
  private-llm-base-url: "http://llm-service:8000/v1"
  private-llm-model: "mistral-7b"
```

## π IP κΈ°λ° μ ‘μ† μ„¤μ •

### π“ **3κ°€μ§€ λ°©λ²•**

#### λ°©λ²• 1: NodePort (κ°€μ¥ κ°„λ‹¨) β­ **μ¶”μ²**

```yaml
# k8s/deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: ci-error-agent-service
spec:
  type: NodePort
  ports:
  - port: 8000
    targetPort: 8000
    nodePort: 30800  # μ™Έλ¶€ μ ‘μ† ν¬νΈ
```

**μ ‘μ† λ°©λ²•:**
```bash
# K8s λ…Έλ“ IP ν™•μΈ
kubectl get nodes -o wide

# μ ‘μ† URL
http://<NODE_IP>:30800

# μμ‹
http://192.168.1.100:30800
```

#### λ°©λ²• 2: LoadBalancer (ν΄λΌμ°λ“ ν™κ²½)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ci-error-agent-service
spec:
  type: LoadBalancer
  ports:
  - port: 8000
    targetPort: 8000
```

**μ ‘μ† λ°©λ²•:**
```bash
# External IP ν™•μΈ
kubectl get svc ci-error-agent-service

# μ ‘μ† URL
http://<EXTERNAL_IP>:8000
```

#### λ°©λ²• 3: ClusterIP + Port Forward (κ°λ°/ν…μ¤νΈ)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ci-error-agent-service
spec:
  type: ClusterIP
  ports:
  - port: 8000
```

**μ ‘μ† λ°©λ²•:**
```bash
# Port Forward
kubectl port-forward svc/ci-error-agent-service 8000:8000

# μ ‘μ† URL
http://localhost:8000
```

## π”§ CI μ‹μ¤ν… μ—°λ™ (IP κΈ°λ°)

### Python μ½”λ“ μ—…λ°μ΄νΈ

```python
# ci_system_example.pyμ—μ„

# NodePort μ‚¬μ© μ‹
analyzer = CIErrorAnalyzer(
    agent_url="http://192.168.1.100:30800"  # K8s λ…Έλ“ IP + NodePort
)

# LoadBalancer μ‚¬μ© μ‹
analyzer = CIErrorAnalyzer(
    agent_url="http://10.20.30.40:8000"  # LoadBalancer External IP
)

# ClusterIP + Port Forward (λ΅μ»¬ ν…μ¤νΈ)
analyzer = CIErrorAnalyzer(
    agent_url="http://localhost:8000"
)
```

### μ΄λ©”μΌμ— ν¬ν•¨λ  μΉμΈ λ§ν¬

```python
# ConfigMapμ—μ„ base-url μ„¤μ •
data:
  base-url: "http://192.168.1.100:30800"  # μ‹¤μ  μ ‘μ† κ°€λ¥ν• IP + ν¬νΈ

# μ΄λ©”μΌμ— ν¬ν•¨λλ” λ§ν¬
approval_url = "http://192.168.1.100:30800/approve/eyJ..."
modify_url = "http://192.168.1.100:30800/modify/eyJ..."
```

## π“‹ μ„¤μ • μ²΄ν¬λ¦¬μ¤νΈ

### Private LLM μ„¤μ •
- [ ] Private LLM μ„λ²„ URL ν™•μΈ
- [ ] λ¨λΈ μ΄λ¦„ ν™•μΈ (llama-3-70b, mistral-7b λ“±)
- [ ] API ν‚¤ ν™•μΈ (ν•„μ”ν• κ²½μ°)
- [ ] `k8s/secrets.yaml`μ— μ„¤μ • μ…λ ¥
- [ ] `k8s/deployment.yaml`μ—μ„ `LLM_PROVIDER=private` μ„¤μ •

### IP κΈ°λ° μ ‘μ† μ„¤μ •
- [ ] K8s λ…Έλ“ IP ν™•μΈ
- [ ] Service νƒ€μ… κ²°μ • (NodePort/LoadBalancer/ClusterIP)
- [ ] NodePort ν¬νΈ κ²°μ • (30000-32767)
- [ ] `k8s/secrets.yaml`μ `base-url` μ„¤μ •
- [ ] λ°©ν™”λ²½ κ·μΉ™ ν™•μΈ

## π§ ν…μ¤νΈ

### Private LLM μ—°κ²° ν…μ¤νΈ

```bash
# Pod λ‚΄λ¶€μ—μ„ ν…μ¤νΈ
kubectl exec -it deployment/ci-error-agent -- python -c "
import os
os.environ['LLM_PROVIDER'] = 'private'
os.environ['PRIVATE_LLM_BASE_URL'] = 'http://your-llm-server:8000/v1'
os.environ['PRIVATE_LLM_MODEL'] = 'llama-3-70b'

from app.llm.provider import get_llm
import asyncio

llm = get_llm()
result = asyncio.run(llm.achain('ν…μ¤νΈ ν”„λ΅¬ν”„νΈ'))
print(result)
"
```

### IP μ ‘μ† ν…μ¤νΈ

```bash
# NodePort μ ‘μ† ν…μ¤νΈ
curl http://192.168.1.100:30800/health

# λ¶„μ„ API ν…μ¤νΈ
curl -X POST http://192.168.1.100:30800/analyze \
  -H "Content-Type: application/json" \
  -d '{"ci_log": "test error"}'
```

## π― μ‹¤μ  μ„¤μ • μμ‹

### μ™„μ „ν• secrets.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # SMTP
  smtp-host: "smtp.company.com"
  smtp-port: "587"
  
  # Agent μ ‘μ† URL (μ‹¤μ  IPλ΅ λ³€κ²½)
  base-url: "http://192.168.1.100:30800"
  
  # Private LLM (μ‹¤μ  μ„λ²„ μ •λ³΄λ΅ λ³€κ²½)
  private-llm-base-url: "http://10.0.0.50:8000/v1"
  private-llm-model: "llama-3-70b"

---
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
type: Opaque
stringData:
  # Private LLM
  private-llm-api-key: "your-actual-api-key"
  
  # JWT
  jwt-secret-key: "super-long-random-secret-key-change-this"
```

### deployment.yaml LLM μ„¤μ •

```yaml
env:
- name: LLM_PROVIDER
  value: "private"  # private LLM μ‚¬μ©
```

## π€ λ°°ν¬

```bash
# 1. μ„¤μ • μμ •
# - k8s/secrets.yamlμ—μ„ Private LLM URL, λ¨λΈ, API ν‚¤ μ„¤μ •
# - base-urlμ„ μ‹¤μ  μ ‘μ† κ°€λ¥ν• IP:ν¬νΈλ΅ μ„¤μ •

# 2. λ°°ν¬
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/postgres.yaml
kubectl apply -f k8s/deployment.yaml

# 3. NodePort ν™•μΈ
kubectl get svc ci-error-agent-service

# 4. λ…Έλ“ IP ν™•μΈ
kubectl get nodes -o wide

# 5. μ ‘μ† ν…μ¤νΈ
# http://<NODE_IP>:30800/health
```

## π’΅ μ¶”μ² μ„¤μ •

### ν”„λ΅λ•μ… ν™κ²½
```yaml
# LLM: Private LLM (μ‚¬λ‚΄ μ„λ²„)
LLM_PROVIDER: private
PRIVATE_LLM_BASE_URL: http://llm-server.company.com:8000/v1

# μ ‘μ†: LoadBalancer (κ³ μ • IP)
Service Type: LoadBalancer
base-url: http://10.20.30.40:8000
```

### κ°λ° ν™κ²½
```yaml
# LLM: OpenAI (λΉ λ¥Έ ν…μ¤νΈ)
LLM_PROVIDER: openai
OPENAI_API_KEY: sk-...

# μ ‘μ†: NodePort (κ°„λ‹¨)
Service Type: NodePort
base-url: http://localhost:30800
```

## β… μ™„λ£ μ²΄ν¬

- [ ] Private LLM μ„λ²„ μ •λ³΄ μ…λ ¥
- [ ] LLM_PROVIDER=private μ„¤μ •
- [ ] Service Type μ„¤μ • (NodePort κ¶μ¥)
- [ ] base-urlμ„ μ‹¤μ  IP:ν¬νΈλ΅ μ„¤μ •
- [ ] λ°°ν¬ λ° ν…μ¤νΈ

μ΄μ  **Private LLMκ³Ό IP κΈ°λ° μ ‘μ†**μ΄ μ™„λ²½ν•κ² μ„¤μ •λμ—μµλ‹λ‹¤! π—β¨

