# Local LLM Server 설정 파일

# Azure OpenAI 설정
azure_openai:
  # API 키 (환경변수 AZURE_OPENAI_API_KEY로도 설정 가능)
  # api_key: "your-azure-openai-api-key-here"
  
  # Base URL (환경변수 AZURE_OPENAI_BASE_URL로도 설정 가능)
  # base_url: "https://your-resource-name.openai.azure.com"
  
  # 배포 이름 (환경변수 AZURE_OPENAI_DEPLOYMENT_NAME로도 설정 가능)
  deployment_name: "gpt-4.1-mini"
  
  # API 버전 (환경변수 AZURE_OPENAI_API_VERSION로도 설정 가능, 선택사항)
  # base_url에 이미 api-version이 포함되어 있다면 생략 가능
  # api_version: "2024-02-15-preview"
  
  # 생성 온도 (0.0 ~ 1.0, 낮을수록 일관성 높음)
  temperature: 0.2
  
  # 최대 토큰 수
  max_tokens: 4096

# 서버 설정
server:
  # 서버 포트 (기본값: 5678, n8n과 동일)
  port: 5678
  
  # 서버 호스트 (0.0.0.0: 모든 인터페이스, 127.0.0.1: 로컬만)
  host: "0.0.0.0"
  
  # 요청 타임아웃 (초)
  timeout: 30

# 로깅 설정
logging:
  # 로그 레벨 (DEBUG, INFO, WARNING, ERROR)
  level: "INFO"
  
  # 로그 파일 (선택사항)
  # file: "local_llm_server.log"

