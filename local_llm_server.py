#!/usr/bin/env python3
"""
Local LLM Server

n8nì„ ëŒ€ì²´í•˜ëŠ” ë¡œì»¬ Python FastAPI ì„œë²„
OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ CI ë¡œê·¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import asyncio
import yaml
from typing import Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import openai
from openai import AsyncOpenAI
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì„¤ì • ë¡œë“œ
def load_config():
    """ì„¤ì • íŒŒì¼ì—ì„œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    config_file = "local_llm_server_config.yaml"
    default_config = {
        "openai": {
            "api_key": os.getenv("OPENAI_API_KEY", ""),
            "model": "gpt-3.5-turbo",
            "temperature": 0.2,
            "max_tokens": 1000
        },
        "server": {
            "port": 5678,
            "host": "0.0.0.0",
            "timeout": 30
        }
    }
    
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                file_config = yaml.safe_load(f) or {}
            # ê¸°ë³¸ ì„¤ì •ì— íŒŒì¼ ì„¤ì • ë³‘í•©
            for key, value in file_config.items():
                if key in default_config:
                    default_config[key].update(value)
                else:
                    default_config[key] = value
        except Exception as e:
            logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
    
    return default_config

config = load_config()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = None
if config["openai"]["api_key"]:
    openai_client = AsyncOpenAI(api_key=config["openai"]["api_key"])
else:
    logger.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ config íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Local LLM Server",
    description="n8nì„ ëŒ€ì²´í•˜ëŠ” ë¡œì»¬ LLM ë¶„ì„ ì„œë²„",
    version="1.0.0"
)

# ìš”ì²­ ëª¨ë¸
class AnalyzeRequest(BaseModel):
    ci_log: str
    symptoms: list
    error_type: str
    context: Optional[str] = None
    repository: Optional[str] = None

# ì‘ë‹µ ëª¨ë¸
class AnalyzeResponse(BaseModel):
    analysis: str
    confidence: float

def calculate_confidence(analysis: str) -> float:
    """ë¶„ì„ ê²°ê³¼ ê¸¸ì´ì— ë”°ë¥¸ ì‹ ë¢°ë„ ê³„ì‚°"""
    length = len(analysis)
    if length > 500:
        return 0.9
    elif length > 300:
        return 0.8
    elif length > 200:
        return 0.7
    elif length > 100:
        return 0.6
    else:
        return 0.5

async def analyze_with_openai(request: AnalyzeRequest) -> Dict[str, Any]:
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ CI ë¡œê·¸ ë¶„ì„"""
    if not openai_client:
        raise HTTPException(
            status_code=503,
            detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ"
        )
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ìë™ì°¨ ì†Œí”„íŠ¸ì›¨ì–´ CI/CD ì˜¤ë¥˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

## ğŸ” ì˜¤ë¥˜ ë¶„ì„

**ì˜¤ë¥˜ ìœ í˜•**: [ì˜¤ë¥˜ ìœ í˜•]
**í•µì‹¬ ì¦ìƒ**: [ì£¼ìš” ì¦ìƒ]

## ğŸ› ï¸ í•´ê²°ì±…

### 1ë‹¨ê³„: [ì²« ë²ˆì§¸ í•´ê²° ë°©ë²•]
[êµ¬ì²´ì ì¸ ì„¤ëª…]

### 2ë‹¨ê³„: [ë‘ ë²ˆì§¸ í•´ê²° ë°©ë²•]
[êµ¬ì²´ì ì¸ ì„¤ëª…]

### 3ë‹¨ê³„: [ì¶”ê°€ í•´ê²° ë°©ë²•]
[êµ¬ì²´ì ì¸ ì„¤ëª…]

**ì°¸ê³ **: [ì¶”ê°€ íŒì´ë‚˜ ì£¼ì˜ì‚¬í•­]

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ, ì‹¤ì œë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”."""

    user_prompt = f"""ë‹¤ìŒ CI ë¡œê·¸ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

**ì˜¤ë¥˜ íƒ€ì…**: {request.error_type}

**ì¦ìƒ**:
{chr(10).join(f"- {symptom}" for symptom in request.symptoms)}

**CI ë¡œê·¸**:
{request.ci_log}

{f"**ì»¨í…ìŠ¤íŠ¸**: {request.context}" if request.context else ""}
{f"**ì €ì¥ì†Œ**: {request.repository}" if request.repository else ""}"""

    try:
        response = await openai_client.chat.completions.create(
            model=config["openai"]["model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=config["openai"]["temperature"],
            max_tokens=config["openai"]["max_tokens"]
        )
        
        analysis = response.choices[0].message.content
        confidence = calculate_confidence(analysis)
        
        return {
            "analysis": analysis,
            "confidence": confidence
        }
        
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=503,
            detail=f"LLM ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        )

@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "service": "Local LLM Server",
        "version": "1.0.0",
        "openai_configured": bool(openai_client)
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "openai_available": bool(openai_client)}

@app.post("/webhook/llm-analyze", response_model=AnalyzeResponse)
async def analyze_ci_error(request: AnalyzeRequest):
    """
    CI ì˜¤ë¥˜ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
    
    n8n webhookê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    logger.info(f"CI ì˜¤ë¥˜ ë¶„ì„ ìš”ì²­: {request.error_type}")
    
    try:
        result = await analyze_with_openai(request)
        logger.info(f"ë¶„ì„ ì™„ë£Œ: ì‹ ë¢°ë„ {result['confidence']}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ì „ì—­ ì˜ˆì™¸ ì²˜ë¦¬"""
    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
    )

if __name__ == "__main__":
    import uvicorn
    
    port = config["server"]["port"]
    host = config["server"]["host"]
    
    logger.info(f"ë¡œì»¬ LLM ì„œë²„ ì‹œì‘: http://{host}:{port}")
    logger.info(f"OpenAI ëª¨ë¸: {config['openai']['model']}")
    
    if not openai_client:
        logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        logger.warning("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ local_llm_server_config.yaml íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    uvicorn.run(
        "local_llm_server:app",
        host=host,
        port=port,
        reload=True,
        log_level="info"
    )

